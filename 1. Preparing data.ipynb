{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data\n",
    "\n",
    "First, we load the data from the original files, knitting together variables and essays.\n",
    "\n",
    "We:\n",
    "* replace the `cntrl_a11_social_class` values with more interpretable names\n",
    "* replace the `cntrl_a11_gender` values with `M` and `F`\n",
    "* replace broken newline characters to identify separate paragraphs\n",
    "\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. We then want to lightly process the essays identifying sentences and tokens. We use a small English spaCy model to do this.\n",
    "1. Since misspellings are really important in the dataset, we wish to try and repair them. We use two strategies: the _enchant_ spell-checker to detect and suggest alternatives (choosing the first option), using character statistics to replace asterisks that indicate transcription errors (data learned from https://github.com/dwyl/english-words). On manual inspection, character context were not really very useful.\n",
    "1. Extract the features, including:\n",
    "  * basic count features over tokens, sentences, asterisk tokens, spelling-replaced tokens\n",
    "  * readability metrics from https://pypi.python.org/pypi/readability\n",
    "  * part-of-speech probabilities\n",
    "  * LIWC counts and probabilities from the lists here http://clpsych.org/shared-task-2018/384-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import csv\n",
    "import gzip\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import pprint\n",
    "import re\n",
    "import time\n",
    "\n",
    "import enchant\n",
    "import spacy\n",
    "from spacy.tokens import Doc, Token\n",
    "import readability\n",
    "import ujson as json\n",
    "\n",
    "from data import read_data, write_data\n",
    "from liwc import CLPSYCH_LIWC\n",
    "from vis import write\n",
    "\n",
    "FORMAT = '%(asctime)-15s\\t%(message)s'\n",
    "logging.basicConfig(format=FORMAT, level=logging.INFO)\n",
    "\n",
    "log = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_exp(patterns):\n",
    "    return re.compile('|'.join(r'(\\b{}\\b)'.format(re.escape(i)) for i in patterns), re.I)\n",
    "\n",
    "EXPERT_EXPS = {}\n",
    "with open('expert.csv') as f:\n",
    "    for row in csv.reader(f):\n",
    "        cat = row[0]\n",
    "        patterns = [r.lower() for r in row[1:]]\n",
    "        EXPERT_EXPS[cat] = make_exp(patterns)\n",
    "LIWC_EXPS = {cat: make_exp(terms) \n",
    "             for cat, terms in CLPSYCH_LIWC.items()}\n",
    "\n",
    "EMBEDDINGS = {}\n",
    "if os.path.exists('fastText.jsonl'):\n",
    "    with open('fastText.jsonl') as f:\n",
    "        for l in f:\n",
    "            i = json.loads(l.strip())\n",
    "            EMBEDDINGS[i['id']] = [round(v, 3) for v in i['vector']]\n",
    "\n",
    "engb = enchant.Dict(\"en_GB\")\n",
    "\n",
    "IGNORE = {\n",
    "    'I',\n",
    "    \"NT\",\n",
    "    \"nt\",\n",
    "    \"alot\",\n",
    "    \"oclock\",\n",
    "    \"etc\",\n",
    "    \"T.V.\",\n",
    "    \"ve\",\n",
    "}\n",
    "# Manually-scanned replacements.\n",
    "REPLS = {\n",
    "    \"n't\": \"'nt\",\n",
    "    \"Iam\": \"I'm\",\n",
    "    \"thay\": \"they\",\n",
    "    \"wen\": \"when\",\n",
    "    \"wud\": \"would\",\n",
    "    \"hav\": \"have\",\n",
    "    'moter': 'motor',\n",
    "    \"vist\": \"visit\",\n",
    "    \"wat\": \"what\",\n",
    "    \"haf\": \"have\",\n",
    "    \"ther\": \"there\",\n",
    "    \"worke\": \"work\",   \n",
    "}\n",
    "DIGIT = re.compile('[0-9]')\n",
    "\n",
    "\n",
    "class Spelling(object):\n",
    "    def __init__(self, cache=True, correct=True):\n",
    "        self.load(cache)\n",
    "        self.correct = correct\n",
    "        \n",
    "    def load(self, cache):\n",
    "        self.cache = {}\n",
    "        if cache:\n",
    "            if os.path.exists('spelling.cache'):\n",
    "                for clean, corrected in pickle.load(open('spelling.cache', 'rb')).items():\n",
    "                    if clean and corrected:\n",
    "                        self.cache[clean] = corrected\n",
    "            \n",
    "    def save(self):\n",
    "        with open('spelling.cache', 'wb') as f:\n",
    "            pickle.dump(self.cache, f)\n",
    "            \n",
    "    def correct_doc(self, doc: Doc):\n",
    "        if not self.correct:\n",
    "            return {\n",
    "                'text': str(doc),\n",
    "                'replacements': []\n",
    "            }\n",
    "        repls = []\n",
    "        for token in doc:\n",
    "            clean, corrected = self.correct_token(token)\n",
    "            if clean != corrected:\n",
    "                repls.append((token.i, token.idx, token.idx + len(clean), \n",
    "                              clean, corrected))\n",
    "        corrected_text = doc.text\n",
    "        replaced_tokens = []\n",
    "        for index, char_start, char_end, original, correction in sorted(repls, reverse=True):\n",
    "            corrected_text = corrected_text[:char_start] + correction + corrected_text[char_end:]\n",
    "            replaced_tokens.append((index, original))\n",
    "        return {\n",
    "            'text': corrected_text, \n",
    "            'replacements': replaced_tokens,\n",
    "        }\n",
    "            \n",
    "    def correct_token(self, token: Token):\n",
    "        clean = token.orth_.strip()\n",
    "        corrected = self.cache.get(clean)\n",
    "        if corrected is not None:\n",
    "            return clean, corrected\n",
    "        \n",
    "        ast_count = clean.count('*')\n",
    "        if clean != '*' and ast_count:\n",
    "            clean = clean.replace('*', '')\n",
    "        suggestions = []\n",
    "        if clean \\\n",
    "           and token.is_alpha \\\n",
    "           and not token.is_currency \\\n",
    "           and not clean.startswith('ANON_') \\\n",
    "           and not DIGIT.search(clean) \\\n",
    "           and not clean.startswith(\"'\") \\\n",
    "           and clean not in IGNORE \\\n",
    "           and not engb.check(clean):\n",
    "            corrected = REPLS.get(clean)\n",
    "            if corrected is None:\n",
    "                suggestions = engb.suggest(clean)\n",
    "        if not suggestions:\n",
    "            corrected = clean\n",
    "        else:\n",
    "            corrected = suggestions[0]\n",
    "        self.cache[clean] = corrected\n",
    "        return clean, corrected\n",
    "\n",
    "\n",
    "def run_shallow_nlp(data: dict):\n",
    "    \"\"\" Takes data blob, returns a dict mapping doc_id to spaCy doc. \"\"\"\n",
    "    nlp = spacy.load('en')\n",
    "    nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "    inputs = []\n",
    "    for s in data.values():\n",
    "        essay = s['essay']\n",
    "        # Replace pounds.\n",
    "        essay = essay.replace('xxxx', 'Â£')\n",
    "        inputs.append((essay, s['id']))\n",
    "    start = time.time()\n",
    "    docs = {}\n",
    "    log.info('Parsing docs')\n",
    "    for doc, doc_id in nlp.pipe(inputs, as_tuples=True, \n",
    "                                disable=['tagger', 'parser', 'ner'], \n",
    "                                batch_size=50):\n",
    "        docs[doc_id] = doc\n",
    "    log.info(f'Parsed {len(docs)} docs in {time.time() - start:.1f}s')\n",
    "    return docs\n",
    "\n",
    "\n",
    "def build_corrections(docs, correct=True):\n",
    "    corrections = {}\n",
    "    start = time.time()\n",
    "    spelling = Spelling(correct=correct)\n",
    "    log.info('Correcting spelling')\n",
    "    for i, (doc_id, doc) in enumerate(docs.items()):\n",
    "        corrections[doc_id] = spelling.correct_doc(doc)\n",
    "    log.info(f'Corrected {len(corrections)} docs in {time.time() - start:.1f}s')\n",
    "    spelling.save()\n",
    "    return corrections\n",
    "\n",
    "\n",
    "def run_nlp(corrections):\n",
    "    docs = {}\n",
    "    nlp = spacy.load('en')\n",
    "    nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "    inputs = [(corrected['text'], doc_id) \n",
    "              for doc_id, corrected in corrections.items()]\n",
    "    start = time.time()\n",
    "    log.info('Parsing docs')\n",
    "    for doc, doc_id in nlp.pipe(inputs, as_tuples=True,\n",
    "                                disable=[\"parser\"],\n",
    "                                batch_size=50):\n",
    "        docs[doc_id] = doc\n",
    "    log.info(f'Parsed {len(corrections)} docs in {time.time() - start:.1f}s')\n",
    "    return docs\n",
    "\n",
    "\n",
    "def extract_doc_features(doc_id, doc, replacements):\n",
    "    d = {}\n",
    "    text = doc.text\n",
    "    n_tokens = len(doc)\n",
    "    n_sents = len(list(doc.sents))\n",
    "    d.update({\n",
    "        'stat_n_tokens': n_tokens,\n",
    "        'stat_n_types': len(set(t.orth_ for t in doc)),\n",
    "        'stat_p_type': len(set(t.orth_ for t in doc)) / n_tokens,\n",
    "        'stat_n_sentences': n_sents,\n",
    "        'stat_mean_sentence': n_tokens / n_sents,\n",
    "    })\n",
    "    \n",
    "    # Spelling, anonymisation features.\n",
    "    ast = lbr = 0\n",
    "    for t in doc:\n",
    "        if '*' in t.orth_:\n",
    "            ast += 1\n",
    "        if '[' in t.orth_:\n",
    "            lbr += 1\n",
    "    d.update({\n",
    "        'noise_p_asttoks': ast / n_tokens,\n",
    "        'noise_p_replacement_tokens': len(replacements) / n_tokens if replacements else 0,\n",
    "        'noise_p_left_bracket': lbr / n_tokens if lbr else 0,\n",
    "    })\n",
    "\n",
    "    # Part-of-speech features.\n",
    "    pos_counts = Counter(t.pos_ for t in doc)\n",
    "    for pos, count in Counter(t.pos_ for t in doc).items():\n",
    "        d.update({\n",
    "            f'syn_p_pos-{pos}': count / n_tokens\n",
    "        })\n",
    "    d[f'syn_r_ADJ_NOUN'] = pos_counts['ADJ'] / pos_counts['NOUN'] \\\n",
    "                           if pos_counts['ADJ'] + pos_counts['NOUN'] else 0\n",
    "\n",
    "    # Readability features.\n",
    "    read = readability.getmeasures([' '.join(t.orth_ for t in s) + '\\n' for s in doc.sents], lang='en')\n",
    "    for cat, metrics in read.items():\n",
    "        for k, v in metrics.items():\n",
    "            d[f'read_{cat.replace(\" \", \"-\")}_{k.replace(\" \", \"-\")}'] = v\n",
    "\n",
    "    # LIWC and expert features.\n",
    "    for supertype, exps in (('LIWC', LIWC_EXPS), ('EXPERT', EXPERT_EXPS)):\n",
    "        for cat, exp in exps.items():\n",
    "            c = len(list(exp.findall(text)))\n",
    "            d[f'{supertype}_zero_{cat}'] = int(c == 0)\n",
    "            d[f'{supertype}_p_{cat}'] = c / n_tokens\n",
    "    \n",
    "    # Entity features.\n",
    "    named_ents = Counter()\n",
    "    typed_ents = Counter()\n",
    "    for e in doc.ents:\n",
    "        named_ents[str(e)] += 1\n",
    "        typed_ents[e.label_] += 1\n",
    "    d['ents_p'] = len(named_ents) / n_tokens if named_ents else 0\n",
    "    for t, c in typed_ents.items():\n",
    "        d[f'ents_p_{t}'] = c / n_tokens if c else 0\n",
    "        \n",
    "    # Embedding features.\n",
    "    global EMBEDDINGS\n",
    "    for i, v in enumerate(EMBEDDINGS.get(doc_id, [])):\n",
    "        d[f'emb_{i}'] = v\n",
    "    return d\n",
    "\n",
    "\n",
    "def add_features(dataset, corrected_docs, corrections):\n",
    "    log.info('Adding features')\n",
    "    start = time.time()\n",
    "    for doc_id, d in dataset.items():\n",
    "        # Basic features.\n",
    "        doc = corrected_docs[doc_id]\n",
    "        replacements = corrections[doc_id]['replacements']\n",
    "        features = extract_doc_features(doc_id, doc, replacements)\n",
    "        d.update(features)\n",
    "        # Make gender numeric.\n",
    "        d['cntrl_gender'] = int(d['cntrl_gender'] == 'F')\n",
    "    log.info(f'Added features in {time.time() - start:.1f}s')\n",
    "    \n",
    "        \n",
    "def dump_csv(data, fname):\n",
    "    # Get cols.\n",
    "    cols = set()\n",
    "    for row in data.values():\n",
    "        cols.update(row.keys())\n",
    "    cols.remove('essay')\n",
    "    cols = list(sorted(cols))\n",
    "    with open(fname, 'w') as f:\n",
    "        w = csv.DictWriter(f, fieldnames=cols)\n",
    "        w.writerow({c: c for c in cols})\n",
    "        for row in data.values():\n",
    "            w.writerow({k: v for k, v in row.items() if k != 'essay'})\n",
    "    \n",
    "\n",
    "def dump_text_for_vectors(docs, fname):\n",
    "    with open(fname, 'w') as f:\n",
    "        for doc_id, doc in docs.items():\n",
    "            f.write(json.dumps({'id': doc_id, 'text': str(doc)}) + '\\n')\n",
    "\n",
    "        \n",
    "def preprocess(data, correct=True):\n",
    "    docs = run_shallow_nlp(data)\n",
    "    corrections = build_corrections(docs, correct=correct)\n",
    "    corrected_docs = run_nlp(corrections)\n",
    "    add_features(data, corrected_docs, corrections)\n",
    "    return data, docs, corrected_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-08 23:47:32,055\tFound 9217 items\t9217 train\n",
      "2018-04-08 23:47:33,419\tParsing docs\n",
      "2018-04-08 23:49:33,072\tParsed 9217 docs in 119.7s\n",
      "2018-04-08 23:49:33,219\tCorrecting spelling\n",
      "2018-04-08 23:49:47,881\tCorrected 9217 docs in 14.8s\n",
      "2018-04-08 23:49:49,369\tParsing docs\n",
      "2018-04-08 23:57:02,896\tParsed 9217 docs in 433.5s\n",
      "2018-04-08 23:57:02,918\tAdding features\n",
      "2018-04-09 00:04:30,872\tAdded features in 448.0s\n"
     ]
    }
   ],
   "source": [
    "train = read_data('../data/clpsych_2018_training_data/clp18_st_train')\n",
    "# train = dict(list(train.items())[:20])\n",
    "log.info(f'Found {len(train)} items\\t{len(train)} train')\n",
    "_, docs, corrected_docs = preprocess(train)\n",
    "write(train, docs, corrected_docs, '../vis/CLPsych18-train')\n",
    "write_data(train, 'train.jsonl')\n",
    "dump_csv(train, 'train.csv')\n",
    "dump_text_for_vectors(corrected_docs, 'train_for_vectors.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-09 00:05:26,439\tFound 1000 items\t1000 test\n",
      "2018-04-09 00:05:28,058\tParsing docs\n",
      "2018-04-09 00:05:45,518\tParsed 1000 docs in 17.5s\n",
      "2018-04-09 00:05:45,669\tCorrecting spelling\n",
      "2018-04-09 00:05:47,942\tCorrected 1000 docs in 2.4s\n",
      "2018-04-09 00:05:49,571\tParsing docs\n",
      "2018-04-09 00:06:55,116\tParsed 1000 docs in 65.5s\n",
      "2018-04-09 00:06:55,153\tAdding features\n",
      "2018-04-09 00:07:57,015\tAdded features in 61.9s\n"
     ]
    }
   ],
   "source": [
    "test = read_data('../data/clpsych_2018_test_data/clp18_st_test')\n",
    "log.info(f'Found {len(test)} items\\t{len(test)} test')\n",
    "_, docs, corrected_docs = preprocess(test)\n",
    "write_data(test, 'test.jsonl')\n",
    "dump_csv(test, 'test.csv')\n",
    "dump_text_for_vectors(corrected_docs, 'test_for_vectors.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-13 22:07:09,050\tFound 9217 items\t9217 train\n",
      "2018-04-13 22:07:11,819\tParsing docs\n",
      "2018-04-13 22:08:26,741\tParsed 9217 docs in 74.9s\n",
      "2018-04-13 22:08:26,858\tCorrecting spelling\n",
      "2018-04-13 22:08:29,171\tCorrected 9217 docs in 2.4s\n",
      "2018-04-13 22:08:30,113\tParsing docs\n",
      "2018-04-13 22:15:59,546\tParsed 9217 docs in 449.4s\n",
      "2018-04-13 22:15:59,595\tAdding features\n",
      "2018-04-13 22:29:44,885\tAdded features in 825.3s\n"
     ]
    }
   ],
   "source": [
    "train = read_data('../data/clpsych_2018_training_data/clp18_st_train')\n",
    "# train = dict(list(train.items())[:20])\n",
    "log.info(f'Found {len(train)} items\\t{len(train)} train')\n",
    "_, docs, corrected_docs = preprocess(train, correct=False)\n",
    "write(train, docs, corrected_docs, '../vis/CLPsych18-train')\n",
    "write_data(train, 'train.raw.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

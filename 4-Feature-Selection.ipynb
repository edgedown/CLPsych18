{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "* -Upsample positives-\n",
    "* -TfidfVectorizer-\n",
    "* -Optimise precision-\n",
    "* -Feature selection using positive-class precision?-\n",
    "* -Try ngrams-\n",
    "* -Try SelectFromModel-\n",
    "* -Replace words with WordNet synset frequencies-\n",
    "* -Try against a binary depresed (>=4) or not label-\n",
    "* Average across multiple trials with bootstrap resampling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastText import train_supervised, train_unsupervised, load_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, f_regression, mutual_info_regression, chi2\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC, SVR\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import ujson as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fname):\n",
    "    data = []\n",
    "    with open(fname) as f:\n",
    "        return [json.loads(l.strip()) for l in f]\n",
    "\n",
    "train = load_data('train.jsonl')\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "for i, d in enumerate(train):\n",
    "    doc = nlp(d['essay'], disable=['parser', 'ner'])\n",
    "    d.update({\n",
    "        'tokens': len(doc),\n",
    "        'types': len(set(t.orth_ for t in doc)),\n",
    "        'type_token_pct': len(set(t.orth_ for t in doc)) / len(doc),\n",
    "        'sentences': len(list(doc.sents)),\n",
    "        'asterisk_words_pct': len([t for t in doc if '*' in t.orth_]) / len(doc),\n",
    "        'asterisk_words_count': len([t for t in doc if '*' in t.orth_]),\n",
    "    })\n",
    "    if i % 500 == 0:\n",
    "        print(f'Processed {i + 1}')\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_make_xy(data, label_name):\n",
    "    X, y = [], []\n",
    "    for i in data:\n",
    "        label = i[label_name]\n",
    "        if label == '':\n",
    "            continue\n",
    "        X.append(i['essay'])\n",
    "        y.append(label)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def text_experiment(data, label_name, scale_y=False):\n",
    "    print(f'Experiment\\t{label_name}')\n",
    "    X, y = text_make_xy(data, label_name)\n",
    "    \n",
    "    pipeline = make_pipeline(\n",
    "        CountVectorizer(ngram_range=(1,3)),\n",
    "        SelectKBest(f_regression, k=100),\n",
    "        #SGDRegressor(max_iter=1000, tol=1e3),\n",
    "        SVR(kernel='rbf', C=1e3, gamma=0.1),\n",
    "    )\n",
    "    grid = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid={},\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=4,\n",
    "        verbose=0,\n",
    "        cv=10,\n",
    "    )\n",
    "    if scale_y:\n",
    "        y = scale(y)\n",
    "    clf = grid.fit(X, y)\n",
    "    m = clf.cv_results_['mean_test_score'][clf.best_index_]\n",
    "    s = clf.cv_results_['std_test_score'][clf.best_index_]\n",
    "    print(f'\\nResults:\\n{m:.3f}±{2*s:.3f}\\t{clf.best_params_}')\n",
    "\n",
    "    features = clf.best_estimator_.steps[0][1].get_feature_names()  # feature names\n",
    "    mask = clf.best_estimator_.steps[1][1].get_support() #list of booleans\n",
    "    weights = clf.best_estimator_.steps[-1][1].coef_  # weights\n",
    "    weighted = [] # The list of your K best features\n",
    "    \n",
    "    for selected, feature_name in zip(mask, features):\n",
    "        if selected:\n",
    "            weighted.append((feature_name, weights[len(weighted)]))\n",
    "    \n",
    "    print('\\nRanked coefficients:')\n",
    "    for k, v in sorted(weighted, key=lambda i: i[1], reverse=True):\n",
    "        print(f'\\t{v:.3f}\\t{k}')\n",
    "\n",
    "    print('\\n')\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = text_experiment(train, 'a23_pdistress', scale_y=False)\n",
    "clf = text_experiment(train, 'a33_pdistress', scale_y=False)\n",
    "clf = text_experiment(train, 'a42_pdistress', scale_y=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(d['a23_pdistress'] for d in train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarise to distressed/not\n",
    "\n",
    "It seems that the few essays with a very high corresponding distress scores heavily influence feature selection under regression. The data manual says that \"a threshold value of 4+ on the 9-item \\[Malaise Inventory\\] scale is generally used to indicate depression\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_make_binary_xy(data, label_name):\n",
    "    X, y = [], []\n",
    "    for i in data:\n",
    "        label = i[label_name]\n",
    "        if label == '':\n",
    "            continue\n",
    "        X.append(i['essay'])\n",
    "        y.append(label>=4)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def text_binary_experiment(data, label_name, scale_y=False):\n",
    "    print(f'Experiment\\t{label_name}')\n",
    "    X, y = text_make_binary_xy(data, label_name)\n",
    "    \n",
    "    #pipeline = make_pipeline(\n",
    "    #    CountVectorizer(), #CountVectorizer(ngram_range=(1,2)),\n",
    "    #    SelectFromModel(LinearSVC(penalty=\"l1\", dual=False, tol=1e-3)),\n",
    "    #    LinearSVC(penalty=\"l2\"),\n",
    "    #)\n",
    "    pipeline = make_pipeline(\n",
    "        TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english'),\n",
    "        #MultinomialNB(alpha=.01),\n",
    "        KNeighborsClassifier()\n",
    "    )\n",
    "    grid = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid={},\n",
    "        scoring='f1',\n",
    "        n_jobs=4,\n",
    "        verbose=0,\n",
    "        cv=10,\n",
    "    )\n",
    "    if scale_y:\n",
    "        y = scale(y)\n",
    "    clf = grid.fit(X, y)\n",
    "    m = clf.cv_results_['mean_test_score'][clf.best_index_]\n",
    "    s = clf.cv_results_['std_test_score'][clf.best_index_]\n",
    "    print(f'\\nResults:\\n{m:.3f}±{2*s:.3f}\\t{clf.best_params_}')\n",
    "\n",
    "    \"\"\"\n",
    "    features = clf.best_estimator_.steps[0][1].get_feature_names()  # feature names\n",
    "    mask = clf.best_estimator_.steps[1][1].get_support() #list of booleans\n",
    "    weights = clf.best_estimator_.steps[-1][1].coef_  # weights\n",
    "    weighted = [] # The list of your K best features\n",
    "    \n",
    "    for selected, feature_name in zip(mask, features):\n",
    "        if selected:\n",
    "            weighted.append((feature_name, weights[0][len(weighted)]))\n",
    "    \n",
    "    print('\\nRanked coefficients:')\n",
    "    for k, v in sorted(weighted, key=lambda i: abs(i[1]), reverse=True):\n",
    "        print(f'\\t{v:.3f}\\t{k}')\n",
    "    \"\"\"\n",
    "\n",
    "    features = clf.best_estimator_.steps[0][1].get_feature_names()  # feature names\n",
    "    weights = clf.best_estimator_.steps[-1][1].coef_  # weights\n",
    "    weighted = zip(features, weights[0])\n",
    "    \n",
    "    print('\\nRanked coefficients:')\n",
    "    for i, (k, v) in enumerate(sorted(weighted, key=lambda i: i[1], reverse=True)):\n",
    "        if i >= 100:\n",
    "            break\n",
    "        print(f'\\t{v:.3f}\\t{k}')\n",
    "\n",
    "    print('\\n')\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = text_binary_experiment(train, 'a23_pdistress', scale_y=False)\n",
    "clf = text_binary_experiment(train, 'a33_pdistress', scale_y=False)\n",
    "clf = text_binary_experiment(train, 'a42_pdistress', scale_y=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection without classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_make_binary_xy(data, label_name):\n",
    "    X, y = [], []\n",
    "    for i in data:\n",
    "        label = i[label_name]\n",
    "        if label == '':\n",
    "            continue\n",
    "        X.append(i['essay'])\n",
    "        y.append(label>=4)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def text_binary_experiment(data, label_name):\n",
    "    print(f'Experiment\\t{label_name}')\n",
    "    X, y = text_make_binary_xy(data, label_name)\n",
    "    \n",
    "    pipeline = make_pipeline(\n",
    "        #CountVectorizer(),\n",
    "        #CountVectorizer(ngram_range=(1,2)),\n",
    "        CountVectorizer(max_df=0.5, min_df=5, stop_words='english'),\n",
    "        #TfidfVectorizer(sublinear_tf=True, max_df=0.5, min_df=10, stop_words='english'),\n",
    "        SelectKBest(chi2, k=100),\n",
    "    )\n",
    "    clf = pipeline.fit(X, y)\n",
    "    print('\\nResults:')\n",
    "\n",
    "    features = clf.steps[0][1].get_feature_names()  # feature names\n",
    "    mask = clf.steps[1][1].get_support() #list of booleans\n",
    "    weights = clf.steps[1][1].scores_\n",
    "    weighted = [] # The list of your K best features\n",
    "    \n",
    "    for selected, feature_name in zip(mask, features):\n",
    "        if selected:\n",
    "            weighted.append((feature_name, weights[len(weighted)]))\n",
    "    \n",
    "    print('\\nRanked coefficients:')\n",
    "    for k, v in sorted(weighted, key=lambda i: abs(i[1]), reverse=True):\n",
    "        print(f'\\t{v:.3f}\\t{k}')\n",
    "\n",
    "    print('\\n')\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = text_binary_experiment(train, 'a23_pdistress')\n",
    "clf = text_binary_experiment(train, 'a33_pdistress')\n",
    "clf = text_binary_experiment(train, 'a42_pdistress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only subjects with consistent distress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_names = [\n",
    "    'a23_pdistress',\n",
    "    'a33_pdistress',\n",
    "    'a42_pdistress',\n",
    "]\n",
    "Counter(all(d[v] != '' and d[v] >= 4 for v in label_names) for d in train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_make_binary_xy(data, label_names):\n",
    "    X, y = [], []\n",
    "    for i in data:\n",
    "        if any(i[l] == '' for l in label_names):\n",
    "            continue\n",
    "        X.append(i['essay'])\n",
    "        y.append(all(i[l] >= 4 for l in label_names))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def text_binary_experiment(data, label_names):\n",
    "    X, y = text_make_binary_xy(data, label_names)\n",
    "    \n",
    "    pipeline = make_pipeline(\n",
    "        #CountVectorizer(),\n",
    "        #CountVectorizer(ngram_range=(1,2)),\n",
    "        CountVectorizer(max_df=0.5, min_df=5, stop_words='english'),\n",
    "        #CountVectorizer(max_df=0.5, min_df=5, stop_words='english', binary=True),\n",
    "        #CountVectorizer(max_df=0.5, min_df=5, stop_words='english', ngram_range=(1,2)),\n",
    "        #TfidfVectorizer(sublinear_tf=True, max_df=0.5, min_df=5, stop_words='english'),\n",
    "        #TfidfVectorizer(sublinear_tf=True, max_df=0.5, min_df=5, stop_words='english', ngram_range=(1,2)),\n",
    "        SelectKBest(chi2, k=100),\n",
    "    )\n",
    "    clf = pipeline.fit(X, y)\n",
    "\n",
    "    features = clf.steps[0][1].get_feature_names()  # feature names\n",
    "    mask = clf.steps[1][1].get_support() #list of booleans\n",
    "    weights = clf.steps[1][1].scores_\n",
    "    weighted = [] # The list of your K best features\n",
    "    \n",
    "    for selected, feature_name in zip(mask, features):\n",
    "        if selected:\n",
    "            weighted.append((feature_name, weights[len(weighted)]))\n",
    "    \n",
    "    print('\\nRanked coefficients:')\n",
    "    for k, v in sorted(weighted, key=lambda i: abs(i[1]), reverse=True):\n",
    "        if v > 2.71:\n",
    "            # independence rejected at 90% confidence with chi2\n",
    "            # https://nlp.stanford.edu/IR-book/html/htmledition/feature-selectionchi2-feature-selection-1.html\n",
    "            print(f'\\t{v:.3f}\\t{k}')\n",
    "\n",
    "    print('\\n')\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = text_binary_experiment(train, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fastText classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_make_binary_xy(data, label_names, train_split=0.75):\n",
    "    X, y = [], []\n",
    "    for i in data:\n",
    "        if any(i[l] == '' for l in label_names):\n",
    "            continue\n",
    "        if all(i[l] >= 3 for l in label_names):\n",
    "            X.append(i['essay'])\n",
    "            y.append(True)\n",
    "        if all(i[l] <= 0 for l in label_names):\n",
    "            X.append(i['essay'])\n",
    "            y.append(False)\n",
    "    return X, y\n",
    "\n",
    "train = load_data('train.jsonl')\n",
    "X, y = text_make_binary_xy(train, label_names)\n",
    "print(len(X), len(y))\n",
    "print(Counter(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (essay, label) in enumerate(zip(X, y)):\n",
    "    if label:\n",
    "        print(f'{i}\\n\\n{essay}\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_ft(X, y, train_split=0.75):\n",
    "    train, dev = [], []\n",
    "    for i, (text, label) in enumerate(zip(X, y)):\n",
    "        line = f'__label__{label} {text}'\n",
    "        if i < train_split * len(X):\n",
    "            train.append(line)\n",
    "        else:\n",
    "            dev.append(line)\n",
    "    return train, dev\n",
    "\n",
    "train_ft, dev_ft = split_ft(X, y)\n",
    "\n",
    "train_path = 'fasttext.train'\n",
    "with open(train_path, 'w') as fh:\n",
    "    fh.write('\\n'.join(train_ft))\n",
    "\n",
    "dev_path = 'fasttext.dev'\n",
    "with open(dev_path, 'w') as fh:\n",
    "    fh.write('\\n'.join(dev_ft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ft_experiment(model, path):\n",
    "    with open(path) as fh:\n",
    "        lines = fh.readlines()\n",
    "    preds = set()  # predicted true\n",
    "    trues = set()  # gold true\n",
    "    total = 0\n",
    "    for i, t in enumerate(lines):\n",
    "        total += 1\n",
    "        labl, text = t.strip().split(maxsplit=1)\n",
    "        if labl == '__label__True':\n",
    "            trues.add(i)\n",
    "        pred = model.predict(text)[0][0]\n",
    "        if pred == '__label__True':\n",
    "            preds.add(i)\n",
    "    n_crct = len(preds.intersection(trues))\n",
    "    n_pred = len(preds)\n",
    "    n_true = len(trues)\n",
    "    print(f'Total data points: {total}')\n",
    "    print(f'Correct/predicted/gold: {n_crct}, {n_pred}, {n_true}')\n",
    "    p = n_crct / n_pred if n_pred else 1.0\n",
    "    r = n_crct / n_true if n_true else 1.0\n",
    "    f = 2 * p * r / (p + r) if p + r > 0 else 0.0\n",
    "    print(f'Precision/recall/f-score: {p:.2}/{r:.2}/{f:.2}')\n",
    "\n",
    "# train_supervised uses the same arguments and defaults as the fastText cli\n",
    "model = train_supervised(\n",
    "    input=train_path, epoch=25, lr=1.0, wordNgrams=2, verbose=2, minCount=1\n",
    ")\n",
    "ft_experiment(model, dev_path)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_supervised uses the same arguments and defaults as the fastText cli\n",
    "model = train_supervised(\n",
    "    input=train_path, epoch=25, lr=1.0, pretrainedVectors='crawl-300d-2M.vec.zip'\n",
    ")\n",
    "ft_experiment(model, dev_path)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify true sizes as a proportion of negatives\n",
    "SIZES = [0.5, 1.0, 2.0, 4.0, 8.0, 16.0]\n",
    "\n",
    "\n",
    "for size in SIZES:\n",
    "    \n",
    "    # split True and False instances\n",
    "    true = []\n",
    "    fool = []\n",
    "    for line in train_ft:\n",
    "        labl, text = line.strip().split(maxsplit=1)\n",
    "        if labl == '__label__True':\n",
    "            true.append(line)\n",
    "        else:\n",
    "            fool.append(line)\n",
    "            \n",
    "    # resample True instances and shuffle\n",
    "    n_fool = len(fool)\n",
    "    n_true = int(size * n_fool)\n",
    "    new = list(np.random.choice(true, size=n_true, replace=True))\n",
    "    new += fool\n",
    "    np.random.shuffle(new)\n",
    "    \n",
    "    # write to fastText-formatted file\n",
    "    path = f'fasttext.train.{int(100*size):0>3}'\n",
    "    with open(path, 'w') as fh:\n",
    "        fh.write('\\n'.join(new))\n",
    "        \n",
    "    # train_supervised uses the same arguments and defaults as the fastText cli\n",
    "    print(f'{path} ({n_true} True / {n_fool} False)')\n",
    "    #model = train_supervised(\n",
    "    #    input=path, epoch=25, lr=1.0, wordNgrams=2, verbose=2, minCount=1\n",
    "    #)\n",
    "    model = train_supervised(\n",
    "        input=train_path, epoch=25, lr=1.0, pretrainedVectors='crawl-300d-2M.vec.zip'\n",
    "    )\n",
    "    ft_experiment(model, dev_path)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 1.0\n",
    "\n",
    "# split True and False instances\n",
    "true = []\n",
    "fool = []\n",
    "for line in train_ft:\n",
    "    labl, text = line.strip().split(maxsplit=1)\n",
    "    if labl == '__label__True':\n",
    "        true.append(line)\n",
    "    else:\n",
    "        fool.append(line)\n",
    "\n",
    "# resample True instances and shuffle\n",
    "n_fool = len(fool)\n",
    "n_true = int(size * n_fool)\n",
    "new = list(np.random.choice(true, size=n_true, replace=True))\n",
    "new += fool\n",
    "np.random.shuffle(new)\n",
    "\n",
    "# write to fastText-formatted file\n",
    "path = f'fasttext.train.{int(100*size):0>3}'\n",
    "with open(path, 'w') as fh:\n",
    "    fh.write('\\n'.join(new))\n",
    "\n",
    "# train_supervised uses the same arguments and defaults as the fastText cli\n",
    "print(f'{path} ({n_true} True / {n_fool} False)')\n",
    "#model = train_supervised(\n",
    "#    input=path, epoch=25, lr=1.0, wordNgrams=2, verbose=2, minCount=1\n",
    "#)\n",
    "model = train_supervised(\n",
    "    input=train_path, epoch=25, lr=1.0, pretrainedVectors='crawl-300d-2M.vec.zip'\n",
    ")\n",
    "ft_experiment(model, dev_path)\n",
    "model.save_model(\"fasttext.bin\")\n",
    "print()\n",
    "\n",
    "#model.quantize(input=train_path, qnorm=True, retrain=True, cutoff=100000)\n",
    "#ft_experiment(model, dev_path)\n",
    "#model.save_model(\"fasttext.ftz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in enumerate(dev):\n",
    "    labl, text = t.strip().split(maxsplit=1)\n",
    "    v = model.get_sentence_vector(text)\n",
    "    print(i, v.shape, v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = train_unsupervised(train_path, pretrainedVectors='crawl-300d-2M.vec.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(train_unsupervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in enumerate(dev):\n",
    "    labl, text = t.strip().split(maxsplit=1)\n",
    "    v = m.get_sentence_vector(text)\n",
    "    print(i, v.shape, v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, line in enumerate(dev):\n",
    "    if line.startswith('__label__True'):\n",
    "        pred = model.predict(line)\n",
    "        print(f'{i}\\t{pred}\\t{line}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace words with synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from spacy.symbols import ADJ, ADV, NOUN, VERB\n",
    "\n",
    "pos_map = {\n",
    "    ADJ: wn.ADJ,\n",
    "    ADV: wn.ADV,\n",
    "    NOUN: wn.NOUN,\n",
    "    VERB: wn.VERB,\n",
    "}\n",
    "\n",
    "def iter_token_hypernyms(token):\n",
    "    for s in wn.synsets(token.lemma_, pos=pos_map[token.pos]):\n",
    "        for h in s.hypernyms():\n",
    "            yield '_'.join(h.name().split('.'))\n",
    "\n",
    "def hypernym_text(text):\n",
    "    for s in nlp(text).sents:\n",
    "        for t in s:\n",
    "            if t.pos not in pos_map:\n",
    "                continue\n",
    "            yield 'WORD: {}'.format(' '.join(iter_token_hypernyms(t)))\n",
    "        yield ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train:\n",
    "    i['hypernym_text'] = '\\n'.join(hypernym_text(i['essay']))\n",
    "\n",
    "print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hyponyms(synset, max_depth=1, indent=' '*4):\n",
    "    def _print_hyponyms(s, i=0):\n",
    "        if i > max_depth:\n",
    "            return\n",
    "        print('{}{} — {}'.format(indent*i, s.name(), ', '.join(s.lemma_names())))\n",
    "        for h in s.hyponyms():\n",
    "            _print_hyponyms(h, i+1)\n",
    "    \n",
    "    synset = '.'.join(synset.rsplit('_', 2))\n",
    "    _print_hyponyms(wn.synset(synset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_make_hypernym_xy(data, label_names):\n",
    "    X, y = [], []\n",
    "    for i in data:\n",
    "        if any(i[l] == '' for l in label_names):\n",
    "            continue\n",
    "        X.append(i['hypernym_text'])\n",
    "        y.append(all(i[l] >= 4 for l in label_names))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def text_hypernym_experiment(data, label_names):\n",
    "    X, y = text_make_hypernym_xy(data, label_names)\n",
    "    \n",
    "    pipeline = make_pipeline(\n",
    "        #CountVectorizer(),\n",
    "        #CountVectorizer(ngram_range=(1,2)),\n",
    "        CountVectorizer(max_df=0.5, min_df=5, stop_words='english'),\n",
    "        #CountVectorizer(max_df=0.5, min_df=5, stop_words='english', binary=True),\n",
    "        #CountVectorizer(max_df=0.5, min_df=5, stop_words='english', ngram_range=(1,2)),\n",
    "        #TfidfVectorizer(sublinear_tf=True, max_df=0.5, min_df=5, stop_words='english'),\n",
    "        #TfidfVectorizer(sublinear_tf=True, max_df=0.5, min_df=5, stop_words='english', ngram_range=(1,2)),\n",
    "        SelectKBest(chi2, k=100),\n",
    "    )\n",
    "    clf = pipeline.fit(X, y)\n",
    "\n",
    "    features = clf.steps[0][1].get_feature_names()  # feature names\n",
    "    mask = clf.steps[1][1].get_support() #list of booleans\n",
    "    weights = clf.steps[1][1].scores_\n",
    "    weighted = [] # The list of your K best features\n",
    "    \n",
    "    for selected, feature_name in zip(mask, features):\n",
    "        if selected:\n",
    "            w = weights[len(weighted)]\n",
    "            weighted.append((feature_name, w))\n",
    "    \n",
    "    print('\\nRanked coefficients:\\n')\n",
    "    for k, v in sorted(weighted, key=lambda i: abs(i[1]), reverse=True):\n",
    "        if v > 2.71:\n",
    "            # independence rejected at 90% confidence with chi2\n",
    "            # https://nlp.stanford.edu/IR-book/html/htmledition/feature-selectionchi2-feature-selection-1.html\n",
    "            print(f'{v:.3f}\\t', end='')\n",
    "            print_hyponyms(k)\n",
    "            print()\n",
    "\n",
    "    print('\\n')\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = text_hypernym_experiment(train, label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
